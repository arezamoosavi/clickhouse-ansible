<?xml version="1.0"?>
<!-- {{ ansible_managed }} -->
<yandex>
    <logger>
        <!-- Possible levels: https://github.com/pocoproject/poco/blob/develop/Foundation/include/Poco/Logger.h#L105 -->
        <level>{{ clickhouse_logger.level }}</level>
        <log>{{ clickhouse_logger.log }}</log>
        <errorlog>{{ clickhouse_logger.errorlog }}</errorlog>
        <size>{{ clickhouse_logger.size }}</size>
        <count>{{ clickhouse_logger.count }}</count>
    </logger>
    
    <http_port>{{ clickhouse_http_port }}</http_port>
    <tcp_port>{{ clickhouse_tcp_port }}</tcp_port>

    <!-- if enabled, configure ecryption settings https://altinity.com/blog/2019/3/5/clickhouse-networking-part-2 -->
    <!-- <https_port>{{ clickhouse_https_port }}</https_port> -->
    <!-- <tcp_port_secure>{{ clickhouse_tcp_secure_port }}</tcp_port_secure> -->

    <interserver_http_port>{{ clickhouse_interserver_http }}</interserver_http_port>
    <interserver_http_host>{{ inventory_hostname }}</interserver_http_host>

    <!-- Which addresses to listen on? At least localhost -->
    {% for host in clickhouse_listen_host %}
    <listen_host>{{ host }}</listen_host>
    {% endfor %}

    <!-- Don't exit if ipv6 or ipv4 unavailable, but listen_host with this protocol specified -->
    <!-- <listen_try>0</listen_try> -->
    <!-- Allow listen on same address:port -->
    <!-- <listen_reuse_port>0</listen_reuse_port> -->
    <!-- <listen_backlog>64</listen_backlog> -->


    <max_connections>{{ clickhouse_config.max_connections }}</max_connections>
    <keep_alive_timeout>{{ clickhouse_config.keep_alive_timeout }}</keep_alive_timeout>

    <!-- Maximum number of concurrent queries. -->
    <max_concurrent_queries>{{ clickhouse_config.max_concurrent_queries }}</max_concurrent_queries>

    <max_server_memory_usage>{{ clickhouse_max_server_memory_usage }}</max_server_memory_usage>
    <max_server_memory_usage_to_ram_ratio>{{ clickhouse_max_server_memory_usage_to_ram_ratio }}</max_server_memory_usage_to_ram_ratio>
    <total_memory_profiler_step>{{ clickhouse_total_memory_profiler_step }}</total_memory_profiler_step>
    <total_memory_tracker_sample_probability>{{ clickhouse_total_memory_tracker_sample_probability }}</total_memory_tracker_sample_probability>
    <mark_cache_size>{{ clickhouse_config.mark_cache_size }}</mark_cache_size>
    <uncompressed_cache_size>{{ clickhouse_config.uncompressed_cache_size }}</uncompressed_cache_size>

    <path>{{ clickhouse_path }}</path>
    <tmp_path>{{ clickhouse_tmp_path }}</tmp_path>
    <user_files_path>{{ clickhouse_user_files_path }}</user_files_path>

    <!-- Path to folder where users and roles created by SQL commands are stored. -->
    <access_control_path>{{ clickhouse_access_control_path }}</access_control_path>

    <!-- Path to configuration file with users, access rights, profiles of settings, quotas. -->
    <users_config>{{ clickhouse_users_config }}</users_config>

    <default_profile>{{ clickhouse_default_profile }}</default_profile>
    <default_database>{{ clickhouse_default_database }}</default_database>
    <timezone>{{ clickhouse_timezone }}</timezone>

    <mlock_executable>{{ clickhouse_config.mlock_status }}</mlock_executable>

    <!-- Replication parameters -->
    <remote_servers incl="clickhouse_remote_servers" />
    <remote_url_allow_hosts>
        <!-- Host should be specified exactly as in URL. The name is checked before DNS resolution.
            Example: "yandex.ru", "yandex.ru." and "www.yandex.ru" are different hosts.
                    If port is explicitly specified in URL, the host:port is checked as a whole.
                    If host specified here without port, any port with this host allowed.
                    "yandex.ru" -> "yandex.ru:443", "yandex.ru:80" etc. is allowed, but "yandex.ru:80" -> only "yandex.ru:80" is allowed.
            If the host is specified as IP address, it is checked as specified in URL. Example: "[2a02:6b8:a::a]".
            If there are redirects and support for redirects is enabled, every redirect (the Location field) is checked.
        -->

        <!-- Regular expression can be specified. RE2 engine is used for regexps.
            Regexps are not aligned: don't forget to add ^ and $. Also don't forget to escape dot (.) metacharacter
            (forgetting to do so is a common source of error).
        -->
    </remote_url_allow_hosts>

    <zookeeper incl="zookeeper-servers" optional="true" />
    <macros incl="macros" optional="true" />
    
    <!-- Reloading interval for embedded dictionaries, in seconds. Default: 3600. -->
    <builtin_dictionaries_reload_interval>3600</builtin_dictionaries_reload_interval>

    <!-- Maximum session timeout, in seconds. Default: 3600. -->
    <max_session_timeout>{{ clickhouse_config.max_session_timeout }}</max_session_timeout>

    <!-- Default session timeout, in seconds. Default: 60. -->
    <default_session_timeout>{{ clickhouse_config.default_session_timeout }}</default_session_timeout>

    <!-- Reloading interval for embedded dictionaries, in seconds. Default: 3600. -->
    <builtin_dictionaries_reload_interval>{{ clickhouse_config.builtin_dictionaries_reload_interval }}</builtin_dictionaries_reload_interval>

    <!-- Query log. Used only for queries with setting log_queries = 1. -->
    <query_log>
        <!-- What table to insert data. If table is not exist, it will be created.
             When query log structure is changed after system update,
              then old table will be renamed and new table will be created automatically.
        -->
        <database>system</database>
        <table>query_log</table>
        <!--
            PARTITION BY expr https://clickhouse.yandex/docs/en/table_engines/custom_partitioning_key/
            Example:
                event_date
                toMonday(event_date)
                toYYYYMM(event_date)
                toStartOfHour(event_time)
        -->
        <partition_by>toYYYYMM(event_date)</partition_by>

        <!-- Instead of partition_by, you can provide full engine expression (starting with ENGINE = ) with parameters,
             Example: <engine>ENGINE = MergeTree PARTITION BY toYYYYMM(event_date) ORDER BY (event_date, event_time) SETTINGS index_granularity = 1024</engine>
          -->

        <!-- Interval of flushing data. -->
        <flush_interval_milliseconds>7500</flush_interval_milliseconds>
    </query_log>

    <!-- Trace log. Stores stack traces collected by query profilers.
         See query_profiler_real_time_period_ns and query_profiler_cpu_time_period_ns settings. -->
    <trace_log>
        <database>system</database>
        <table>trace_log</table>

        <partition_by>toYYYYMM(event_date)</partition_by>
        <flush_interval_milliseconds>7500</flush_interval_milliseconds>
    </trace_log>

    <!-- Query thread log. Has information about all threads participated in query execution.
         Used only for queries with setting log_query_threads = 1. -->
    <query_thread_log>
        <database>system</database>
        <table>query_thread_log</table>
        <partition_by>toYYYYMM(event_date)</partition_by>
        <flush_interval_milliseconds>7500</flush_interval_milliseconds>
    </query_thread_log>

    <!-- Metric log contains rows with current values of ProfileEvents, CurrentMetrics collected with "collect_interval_milliseconds" interval. -->
    <metric_log>
        <database>system</database>
        <table>metric_log</table>
        <flush_interval_milliseconds>7500</flush_interval_milliseconds>
        <collect_interval_milliseconds>1000</collect_interval_milliseconds>
    </metric_log>

    <!--
        Asynchronous metric log contains values of metrics from
        system.asynchronous_metrics.
    -->
    <asynchronous_metric_log>
        <database>system</database>
        <table>asynchronous_metric_log</table>
        <!--
            Asynchronous metrics are updated once a minute, so there is
            no need to flush more often.
        -->
        <flush_interval_milliseconds>60000</flush_interval_milliseconds>
    </asynchronous_metric_log>

    <dictionaries_config>*_dictionary.xml</dictionaries_config>

    <!-- Allow to execute distributed DDL queries (CREATE, DROP, ALTER, RENAME) on cluster.
         Works only if ZooKeeper is enabled. Comment it if such functionality isn't required. -->
    <distributed_ddl>
        <!-- Path in ZooKeeper to queue with DDL queries -->
        <path>/clickhouse/task_queue/ddl</path>

        <!-- Settings from this profile will be used to execute DDL queries -->
        <!-- <profile>default</profile> -->
    </distributed_ddl>

    <!-- Protection from accidental DROP.
         If size of a MergeTree table is greater than max_table_size_to_drop (in bytes) than table could not be dropped with any DROP query.
         If you want do delete one table and don't want to change clickhouse-server config, you could create special file <clickhouse-path>/flags/force_drop_table and make DROP once.
         By default max_table_size_to_drop is 50GB; max_table_size_to_drop=0 allows to DROP any tables.
         The same for max_partition_size_to_drop.
         Uncomment to disable protection.-->
    <!-- <max_table_size_to_drop>0</max_table_size_to_drop> -->
    <!-- <max_partition_size_to_drop>0</max_partition_size_to_drop> -->

    <format_schema_path>{{ clickhouse_format_schema_path }}</format_schema_path>

    <disable_internal_dns_cache>{{ clickhouse_internal_dns_disabled }}</disable_internal_dns_cache>

    <!-- Settings to fine tune MergeTree tables. See documentation in source code, in MergeTreeSettings.h -->
{% if clickhouse.merge_tree_config is defined %}
    <merge_tree>
{% for config in clickhouse.merge_tree_config %}
        <{{ config }}>{{ clickhouse.merge_tree_config[config] }}</{{ config }}>
{% endfor %}
    </merge_tree>
{% endif %}
</yandex>
